{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f4fab7-53c7-4de2-999a-eddc2f14e761",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9236934f-0ac0-4336-8dd4-74ad8a0ef45b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/amitfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/storage/modules/packages/anaconda/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/storage/modules/packages/anaconda/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, AdamW, LlamaTokenizer, LlamaModel, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from nlpaug.augmenter.word import SynonymAug, BackTranslationAug\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "# Evaluating and plotting \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6010e-330b-44fe-8c63-848ac0309399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SynonymAug, BackTranslationAug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c277c8-0f0a-4e27-91e3-b0331000543c",
   "metadata": {},
   "source": [
    "## ***MLM task***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc2663-1bc2-439e-aa92-5801d7f2bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit_opinion_PSE_ISR.csv\",  on_bad_lines='skip') # correct line to load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d20a84-34e5-4046-a397-dc0bb446b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_100_000 = df.sample(n=100_000, random_state=209122282)\n",
    "sub_df_300_000 = df.sample(n=300_000, random_state=209122282)\n",
    "sub_df_500_000 = df.sample(n=500_000, random_state=209122282)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3f67a-70c2-4050-9bcc-ca97020f1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['self_text'].iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da5273-f266-4f1c-a697-a8248ca78716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize(examples):\n",
    "    texts = [text for text in examples['self_text'] if isinstance(text, str)]\n",
    "    return tokenizer(texts, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "sub_df_100_000 = sub_df_100_000.reset_index(drop=True)  # Reset index and drop the old one\n",
    "sub_df_100_000 = sub_df_100_000.drop('Unnamed: 0', axis=1)  # Remove the problematic column\n",
    "dataset_100_000 = Dataset.from_pandas(sub_df_100_000)\n",
    "tokenized_dataset_100_000 = dataset_100_000.map(tokenize, batched=True, remove_columns=dataset_100_000.column_names)\n",
    "tokenized_dataset_100_000.save_to_disk('./tokenized_reddit_data_100_000')\n",
    "print(\"done 100_000\")\n",
    "\n",
    "sub_df_300_000 = sub_df_300_000.reset_index(drop=True)  # Reset index and drop the old one\n",
    "sub_df_300_000 = sub_df_300_000.drop('Unnamed: 0', axis=1)  # Remove the problematic column\n",
    "dataset_300_000 = Dataset.from_pandas(sub_df_300_000)\n",
    "tokenized_dataset_300_000 = dataset_300_000.map(tokenize, batched=True, remove_columns=dataset_300_000.column_names)\n",
    "tokenized_dataset_300_000.save_to_disk('./tokenized_reddit_data_300_000')\n",
    "print(\"done 300_000\")\n",
    "\n",
    "sub_df_500_000 = sub_df_500_000.reset_index(drop=True)  # Reset index and drop the old one\n",
    "sub_df_500_000 = sub_df_500_000.drop('Unnamed: 0', axis=1)  # Remove the problematic column\n",
    "dataset_500_000 = Dataset.from_pandas(sub_df_500_000)\n",
    "tokenized_dataset_500_000 = dataset_500_000.map(tokenize, batched=True, remove_columns=dataset_500_000.column_names)\n",
    "tokenized_dataset_500_000.save_to_disk('./tokenized_reddit_data_500_000')\n",
    "print(\"done 500_000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12f2670b-304a-4534-af06-47fe22b87e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "data_set_size = '300_000'\n",
    "loaded_dataset = load_from_disk(f'./tokenized_reddit_data_{data_set_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ca68872c-a2bf-4d12-9df9-11bc51c708d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e273a7a-d21b-476b-adde-b2d9dae7a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Create data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Standard masking probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e8ce1670-3a2f-4a9c-8931-c04a37dca81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 299997\n",
      "Training set size: 269998\n",
      "Validation set size: 29999\n",
      "Splitting dataset into train and validation sets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdac2ae4640a4629a53ffb9c86c8903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n",
      "Creating validation dataset...\n",
      "Saving split datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/269905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30092 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete!\n"
     ]
    }
   ],
   "source": [
    "# Calculate split sizes\n",
    "total_size = len(tokenized_dataset)\n",
    "val_size = int(total_size * 0.1)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Validation set size: {val_size}\")\n",
    "\n",
    "# Initialize empty lists for train and validation indices\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "# Create batched index assignment with progress bar\n",
    "batch_size = 10000\n",
    "num_batches = math.ceil(total_size / batch_size)\n",
    "\n",
    "print(\"Splitting dataset into train and validation sets...\")\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, total_size)\n",
    "    \n",
    "    # Generate random numbers for this batch\n",
    "    batch_indices = np.random.rand(end_idx - start_idx)\n",
    "    \n",
    "    # Assign indices based on split ratio\n",
    "    for j, rand_val in enumerate(batch_indices):\n",
    "        if rand_val < 0.1:  # 10% for validation\n",
    "            val_indices.append(start_idx + j)\n",
    "        else:\n",
    "            train_indices.append(start_idx + j)\n",
    "\n",
    "\n",
    "# Create the split datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = tokenized_dataset.select(train_indices)\n",
    "print(\"Creating validation dataset...\")\n",
    "val_dataset = tokenized_dataset.select(val_indices)\n",
    "\n",
    "# Optional: Save the split datasets\n",
    "print(\"Saving split datasets...\")\n",
    "train_dataset.save_to_disk(f'./train_dataset_{data_set_size}')\n",
    "val_dataset.save_to_disk(f'./val_dataset_{data_set_size}')\n",
    "\n",
    "print(\"Split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3b408eeb-14c8-4a5f-a248-023444c1dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for MLM evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Identify valid positions (non-padding and actually masked tokens)\n",
    "    mask_positions = labels != -100\n",
    "    \n",
    "    if not np.any(mask_positions):\n",
    "        return {\"mlm_accuracy\": 0.0}\n",
    "    \n",
    "    # Calculate accuracy only on masked positions\n",
    "    correct_predictions = predictions[mask_positions] == labels[mask_positions]\n",
    "    accuracy = correct_predictions.mean()\n",
    "    \n",
    "    # Add more detailed metrics\n",
    "    return {\n",
    "        \"mlm_accuracy\": float(accuracy),\n",
    "        \"num_masked_tokens\": int(mask_positions.sum()),\n",
    "        \"num_correct_predictions\": int(correct_predictions.sum())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a5c41282-2d4f-4394-ac8c-1d6a0e8beeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./roberta-reddit-mlm-final_300_000\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,    # Increased eval batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,                  # More frequent logging\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,                    # Less frequent saving\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5000,                     # More frequent evaluation\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=32,           # Increased eval accumulation\n",
    "    log_level=\"info\",                      # Ensure logs appear\n",
    "    report_to=\"none\"                       # Prevent logging to external tools\n",
    ")\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            print(f\"Step: {state.global_step}, Logs: {logs}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4dfc8-56de-4e7a-9d1d-5475a0d366e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.add_callback(LossLoggerCallback())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "training_output = trainer.train()\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training metrics: {training_output.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce72e3ab-55f6-4523-ba46-2b2d5211489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./roberta-reddit-mlm-final_300_000\n",
      "Configuration saved in ./roberta-reddit-mlm-final_300_000/config.json\n",
      "Model weights saved in ./roberta-reddit-mlm-final_300_000/pytorch_model.bin\n",
      "tokenizer config file saved in ./roberta-reddit-mlm-final_300_000/tokenizer_config.json\n",
      "Special tokens file saved in ./roberta-reddit-mlm-final_300_000/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./roberta-reddit-mlm-final_300_000/tokenizer_config.json',\n",
       " './roberta-reddit-mlm-final_300_000/special_tokens_map.json',\n",
       " './roberta-reddit-mlm-final_300_000/vocab.json',\n",
       " './roberta-reddit-mlm-final_300_000/merges.txt',\n",
       " './roberta-reddit-mlm-final_300_000/added_tokens.json')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both the model and tokenizer\n",
    "output_dir = f\"./roberta-reddit-mlm-final_{data_set_size}\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0379a8-6426-4c2a-bbf7-e5bb03c5f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "output_dir = 'roberta-reddit-mlm-final_300_000/'\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    output_dir, \n",
    "    num_labels=3 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22799b23-b5f8-4193-8d63-8913d0b638b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path_train = 'train_data_new.csv'\n",
    "file_path_test = 'test_data_new.csv'\n",
    "\n",
    "train_df = pd.read_csv(file_path_train)\n",
    "test_df = pd.read_csv(file_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5a19ddc6-90ea-423e-b3a5-98d83f83992a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(888, 222)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7bd1d048-dbc7-4cc6-9909-bb247f06d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(texts, labels, tokenizer, test=False):\n",
    "    # Convert labels to float first\n",
    "    labels = [float(label) for label in labels]\n",
    "    # Train-test split\n",
    "    if test:\n",
    "        train_texts, val_texts, train_labels, val_labels = texts, texts, labels, labels\n",
    "    else:\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "        \n",
    "    # Initialize LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Convert the float labels to the desired mapping: -1.0 -> 0, 0.0 -> 1, 1.0 -> 2\n",
    "    mapping = {-1.0: 0, 0.0: 1, 1.0: 2}\n",
    "    train_labels = [mapping[label] for label in train_labels]\n",
    "    val_labels = [mapping[label] for label in val_labels]\n",
    "    \n",
    "    # Now fit and transform with these mapped labels\n",
    "    train_labels = label_encoder.fit_transform(train_labels)\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    \n",
    "    # Save the label mapping\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    return train_encodings, val_encodings, train_labels, val_labels, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "61346ec8-5645-4284-9605-8647e49f8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-classification\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    # Early stopping related arguments\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Metric to track for saving best model\n",
    "    greater_is_better=False,       # Set to False since we want to minimize loss\n",
    ")\n",
    "\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e47c167-2cd1-4694-827e-76c2209dcc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-reddit-mlm-final_300_000\")\n",
    "\n",
    "# Your texts and labels lists should be defined here\n",
    "texts = train_df['self_text'].tolist()\n",
    "labels = train_df['Label'].tolist()\n",
    "\n",
    "# Preprocess data\n",
    "train_encodings, val_encodings, train_labels, val_labels, label_mapping = preprocess_data(texts, labels, tokenizer, False)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d756554-1be6-444c-b6bf-e636c3af7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer-\n",
    "trained_model_path = \"roberta_after_mlm_finetuned_300.pt\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_classification_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(trained_model_path)\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69af8e41-47d2-4049-91df-4ffe79e67e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['self_text'].tolist()\n",
    "test_labels = test_df['Label'].tolist()\n",
    "\n",
    "test_encodings, _, test_labels, _, mapping = preprocess_data(test_texts, test_labels, tokenizer, test=True)\n",
    "test_dataset = RedditDataset(test_encodings, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1af92-0497-4ec5-b737-545ca33513cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved directory\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained('roberta-classification/checkpoint-534')\n",
    "\n",
    "# Create trainer for the loaded model\n",
    "eval_trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./eval_results\",\n",
    "        per_device_eval_batch_size=4,\n",
    "    ),\n",
    "    compute_metrics=compute_classification_metrics\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = eval_trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(results)\n",
    "\n",
    "# Get predictions\n",
    "predictions = eval_trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee1e8f-f134-4926-b1d1-9a4e2fc65750",
   "metadata": {},
   "source": [
    "# ***Preprocess and try data augmentation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445640bb-e7c4-4d98-aa43-26a1e0870b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path_train = 'train_data.csv'\n",
    "file_path_test = 'test_data.csv'\n",
    "\n",
    "train_df = pd.read_csv(file_path_train)\n",
    "test_df = pd.read_csv(file_path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b12966-47bb-4f18-85bd-8bfb64482fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "# Initialize with just the essential settings\n",
    "syn_aug = SynonymAug(\n",
    "    aug_src='wordnet',\n",
    "    lang='eng'\n",
    ")\n",
    "\n",
    "# Augment data\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "for text, label in tqdm(zip(train_df['self_text'].tolist()[:-100], train_df['Label'].tolist()[:-100])):\n",
    "    aug_syn = syn_aug.augment(text)[0]\n",
    "    augmented_texts.append(aug_syn)\n",
    "    augmented_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0900f-a4c4-46dd-8c69-922afae07989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Augment data\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "for text, label in tqdm(zip(train_df['self_text'].tolist()[:-100], train_df['Label'].tolist()[:-100])):\n",
    "    aug_syn = syn_aug.augment(text)[0]\n",
    "    augmented_texts.append(aug_syn)\n",
    "    augmented_labels.append(label)\n",
    "\n",
    "# Combine original and augmented data\n",
    "augmention_training_df = pd.DataFrame({'self_text': augmented_texts + train_df['self_text'].tolist(), 'Label': augmented_labels + train_df['Label'].tolist()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee9252-e6af-433b-bf39-4d9ad55f3da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['self_text', 'Label']]\n",
    "test_df = test_df[['self_text', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92dd9d-03e4-4755-9180-9cf0d2f18502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(train_df), len(augmention_training_df), len(test_df)\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3427844-55e2-467d-a331-8be136d53508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace 'text_column' and 'label_column' with actual column names\n",
    "texts = train_df['self_text'].tolist()\n",
    "labels = train_df['Label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08c5a828-66d2-4855-baf6-606e9b62373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-reddit-mlm-final_100_000\")\n",
    "\n",
    "# Your texts and labels lists should be defined here\n",
    "texts = train_df['self_text'].tolist()\n",
    "labels = train_df['Label'].tolist()\n",
    "\n",
    "# Preprocess data\n",
    "train_encodings, val_encodings, train_labels, val_labels, label_mapping = preprocess_data(texts, labels, tokenizer, True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e53405-138a-4349-ad73-b9cea0d7d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(trained_model_path)\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf9286c-1817-402a-98a4-8ca5c0c5a103",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Gradual Unfreezing***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27328495-dddd-42a0-9a2d-f5c88e7799f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ***Gradual Unfreezing Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fb3aa-7ae0-40e0-a5cb-84cf75cf98c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(texts, labels, tokenizer, augmention=True, test=False):\n",
    "    # Convert labels to float first\n",
    "    labels = [float(label) for label in labels]\n",
    "    \n",
    "    # Train-test split\n",
    "    if test:\n",
    "        train_texts, val_texts, train_labels, val_labels = texts, texts, labels, labels\n",
    "    elif augmention: \n",
    "        train_texts, val_texts, train_labels, val_labels = texts[:-100], texts[-100:], labels[:-100], labels[-100:]\n",
    "    else:\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "        \n",
    "    # Initialize LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Convert the float labels to the desired mapping: -1.0 -> 0, 0.0 -> 1, 1.0 -> 2\n",
    "    mapping = {-1.0: 0, 0.0: 1, 1.0: 2}\n",
    "    train_labels = [mapping[label] for label in train_labels]\n",
    "    val_labels = [mapping[label] for label in val_labels]\n",
    "    \n",
    "    # Now fit and transform with these mapped labels\n",
    "    train_labels = label_encoder.fit_transform(train_labels)\n",
    "    val_labels = label_encoder.transform(val_labels)\n",
    "    \n",
    "    # Save the label mapping\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    return train_encodings, val_encodings, train_labels, val_labels, label_mapping\n",
    "\n",
    "\n",
    "# Modified optimizer creation function to include weight decay\n",
    "def create_optimizer_with_discriminative_fine_tuning(model, base_lr=2e-5):\n",
    "    layer_parameters = []\n",
    "    \n",
    "    # Add embeddings with lowest learning rate\n",
    "    layer_parameters.append({\n",
    "        'params': model.distilbert.embeddings.parameters(),\n",
    "        'lr': base_lr/2.6\n",
    "    })\n",
    "    \n",
    "    # Add transformer layers with gradually increasing learning rate\n",
    "    for layer_idx in range(len(model.distilbert.transformer.layer)):\n",
    "        layer = model.distilbert.transformer.layer[layer_idx]\n",
    "        layer_parameters.append({\n",
    "            'params': layer.parameters(),\n",
    "            'lr': base_lr/(2.6 - (layer_idx * 0.3))\n",
    "        })\n",
    "    \n",
    "    # Classification head gets the highest learning rate\n",
    "    layer_parameters.append({\n",
    "        'params': model.classifier.parameters(),\n",
    "        'lr': base_lr\n",
    "    })\n",
    "    \n",
    "    # Add weight decay to optimizer\n",
    "    return AdamW(layer_parameters, weight_decay=0.1)\n",
    "\n",
    "\n",
    "def train_epoch(model, train_dataloader, val_dataloader, optimizer, num_epochs, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, \n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss = {total_loss/len(train_dataloader):.4f}, Val Loss = {val_loss/len(val_dataloader):.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                return True  # Signal early stopping\n",
    "    \n",
    "    return False  # No early stopping triggered\n",
    "\n",
    "# Training functions\n",
    "def train_with_gradual_unfreezing(model, train_dataloader, val_dataloader, epochs_per_unfreeze=3):\n",
    "    # Add dropout to classification layer\n",
    "    model.classifier.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    # First freeze all layers except the classifier\n",
    "    for param in model.distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Train only the classifier first with weight decay\n",
    "    optimizer = AdamW(model.classifier.parameters(), lr=2e-5, weight_decay=0.1)\n",
    "    early_stop = train_epoch(model, train_dataloader, val_dataloader, optimizer, epochs_per_unfreeze)\n",
    "    if early_stop:\n",
    "        return model\n",
    "    \n",
    "    # Only unfreeze the top 3 layers (instead of all 6)\n",
    "    for layer_idx in reversed(range(4, 6)): \n",
    "        print(f\"\\nUnfreezing layer {layer_idx}\")\n",
    "        \n",
    "        # Unfreeze current layer\n",
    "        for param in model.distilbert.transformer.layer[layer_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Create optimizer with weight decay\n",
    "        optimizer = create_optimizer_with_discriminative_fine_tuning(model)\n",
    "        early_stop = train_epoch(model, train_dataloader, val_dataloader, optimizer, epochs_per_unfreeze)\n",
    "        if early_stop:\n",
    "            break\n",
    "    \n",
    "    # Load best model before returning\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    return model\n",
    "        \n",
    "def evaluate_model(model, test_dataset, label_mapping):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actual_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert numeric labels back to original classes\n",
    "    inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    predictions = [inv_label_mapping[pred] for pred in predictions]\n",
    "    actual_labels = [inv_label_mapping[label] for label in actual_labels]\n",
    "    \n",
    "    return predictions, actual_labels\n",
    "\n",
    "\n",
    "# Convert to PyTorch Dataset\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "\n",
    "def main():\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Your texts and labels lists should be defined here\n",
    "    texts = train_df['self_text'].tolist()\n",
    "    labels = train_df['Label'].tolist()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_encodings, val_encodings, train_labels, val_labels, label_mapping = preprocess_data(texts, labels, tokenizer, True)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "    val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=len(set(train_labels))\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_with_gradual_unfreezing(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        epochs_per_unfreeze=10\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Optional: Evaluate on test set\n",
    "    # test_predictions, test_actual = evaluate_model(model, test_dataset, label_mapping)\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, label_mapping = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79cd82-e3a8-4a85-b65a-9ff00b848431",
   "metadata": {},
   "source": [
    "## ***Gradual Unfreezing Evaluating***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244daaa0-115c-46d0-8140-d51b70814986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataset, label_mapping, batch_size=16):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Store predictions, labels, and probabilities\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Create label name mapping\n",
    "    label_names = {\n",
    "        0: \"Pro-Palestine\",\n",
    "        1: \"Neutral\",\n",
    "        2: \"Pro-Israel\"\n",
    "    }\n",
    "    \n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Convert numeric predictions and labels to label names\n",
    "    predictions_labels = [label_names[pred] for pred in all_predictions]\n",
    "    true_labels = [label_names[label] for label in all_labels]\n",
    "    \n",
    "    # List of label names in order\n",
    "    all_classes = [\"Pro-Palestine\\n(-1)\", \"Neutral\\n(0)\", \"Pro-Israel\\n(1)\"]\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions_labels, labels=all_classes, zero_division=0))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(true_labels, predictions_labels, labels=all_classes)\n",
    "    \n",
    "    # Normalize the confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Plot both raw and normalized confusion matrices\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Raw Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d',\n",
    "                xticklabels=all_classes, yticklabels=all_classes, ax=ax1,\n",
    "                cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "    ax1.set_title('Confusion Matrix (Raw Counts)', pad=20)\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "    # Normalized Confusion Matrix\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2',\n",
    "                xticklabels=all_classes, yticklabels=all_classes, ax=ax2,\n",
    "                cmap='Blues', cbar_kws={'label': 'Proportion'})\n",
    "    ax2.set_title('Confusion Matrix (Normalized)', pad=20)\n",
    "    ax2.set_ylabel('True Label')\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(all_predictions)\n",
    "    print(all_labels)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Preprocess data\n",
    "train_encodings, val_encodings, train_labels, val_labels, label_mapping = preprocess_data(texts, labels, tokenizer, True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(set(train_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d35f0b-4aa6-4762-a23d-e79ecc70e562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have your test data prepared similarly to train/val data:\n",
    "# Your texts and labels lists should be defined here\n",
    "test_texts = test_df['self_text'].tolist()\n",
    "test_labels = test_df['Label'].tolist()\n",
    "\n",
    "test_encodings, _, test_labels, _, mapping = preprocess_data(test_texts, test_labels, tokenizer, augmention=False, test=True)\n",
    "test_dataset = RedditDataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt', weights_only=True))\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_model(model, test_dataset, label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4319d-f6e5-4d52-a958-e11a598b531d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# RedditBerTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Fan-s/reddit-tc-bert\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154d373-a2e4-4a0b-b0ea-f4b4c3845b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings, val_encodings, train_labels, val_labels = preprocess_data(texts, labels, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744f406-1152-41d5-8430-43a71c10efe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c48b09-c717-4666-8c46-95368b0687e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ***Finetuning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3caf7c-9428-49ed-adb2-5cdf22b1adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d4127-da63-415d-9433-3592b4b54c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(set(labels)))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    # Early stopping related arguments\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Metric to track for saving best model\n",
    "    greater_is_better=False,       # Set to False since we want to minimize loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7fc8e-c3d9-41d0-b64b-3a8ae2ddb06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709033e5-6902-4caa-abf3-9777094fb718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings, val_encodings, train_labels, val_labels, label_mapping = preprocess_data(texts, labels, tokenizer, True)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47780eb6-4de0-4731-bf7f-b1d9693e3ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trained_model_path = \"distilbert_model_finetuned.pt\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(trained_model_path)\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a656ba-070c-4f1a-8f69-3a461fec381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df['self_text'].tolist()\n",
    "test_labels = test_df['Label'].tolist()\n",
    "\n",
    "test_encodings, _, test_labels, _, mapping = preprocess_data(test_texts, test_labels, tokenizer, augmention=False, test=True)\n",
    "test_dataset = RedditDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15e6e0-e524-4de2-aac3-6cf0050a1179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the saved directory\n",
    "loaded_model = DistilBertForSequenceClassification.from_pretrained('distilbert_model_finetuned')\n",
    "\n",
    "# Create trainer for the loaded model\n",
    "eval_trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./eval_results\",\n",
    "        per_device_eval_batch_size=16,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = eval_trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(results)\n",
    "\n",
    "# Get predictions\n",
    "predictions = eval_trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88eba7-75dd-4b11-aca9-f9280b9d310f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = predictions.label_ids\n",
    "\n",
    "# Correct label mapping\n",
    "label_names = {\n",
    "    0: \"Pro-Palestine\",\n",
    "    1: \"Neutral\",\n",
    "    2: \"Pro-Israel\"\n",
    "}\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,8))\n",
    "\n",
    "# Plot raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', \n",
    "            xticklabels=label_names.values(), \n",
    "            yticklabels=label_names.values(),\n",
    "            ax=ax1)\n",
    "ax1.set_title('Confusion Matrix (Raw Counts)')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# Plot normalized values\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2', \n",
    "            xticklabels=label_names.values(), \n",
    "            yticklabels=label_names.values(),cmap='Blues', cbar_kws={'label': 'Proportion'},\n",
    "            ax=ax2)\n",
    "ax2.set_title('Confusion Matrix (Normalized)')\n",
    "ax2.set_ylabel('True Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(labels, preds, \n",
    "                          target_names=list(label_names.values()),\n",
    "                          zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
